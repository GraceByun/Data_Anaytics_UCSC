{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this exercise is to train you in debugging networks using the good old print function and also tensorboard. To simulate poor training, we will train a multilayer perceptron using the CIFAR data.\n",
    "\n",
    "1. Use the CIFAR data set reader from the first homework and read the CIFAR-10 files again. \n",
    "2. Apply random noise to the image \n",
    "3. Convert the image to float and scale to [0.0, 1.0] by dividing the pixel values by the highest pixel value.\n",
    "4. Convert all labels to onehot encoding\n",
    "5. Build a 3-layer multilayer perceptron of size [512, 256, 128]. \n",
    "6. Create a tensorboard summary for plotting the histogram of the weights of the three layers.\n",
    "7. Also write the cost / loss at the end of each epoch to tensorboard.\n",
    "8. Train the network with learning rates of [0.1, 0.01, 0.001]. You will notice that the network will not converge well.\n",
    "9. Submit the snapshot of the histograms for the three learning rates. Describe your observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import operator\n",
    "import functools\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from itertools import chain\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3072\n"
     ]
    }
   ],
   "source": [
    "noofepochs = 128\n",
    "batch_size = 16\n",
    "number_images_batch = 10000\n",
    "image_shape = (32, 32, 3,)\n",
    "image_size = functools.reduce(operator.mul, image_shape)\n",
    "print(image_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_batch_1\n",
      "(10000, 3072)\n",
      "data_batch_2\n",
      "(10000, 3072)\n",
      "data_batch_3\n",
      "(10000, 3072)\n",
      "data_batch_4\n",
      "(10000, 3072)\n",
      "data_batch_5\n",
      "(10000, 3072)\n",
      "(50000, 3072) <class 'list'>\n",
      "50000\n"
     ]
    }
   ],
   "source": [
    "# Read all data\n",
    "alldata = np.zeros((5*number_images_batch, image_size), dtype=np.int)\n",
    "alllabels = []\n",
    "begin = 0\n",
    "end = number_images_batch\n",
    "for filename in glob.glob(\"data_batch*\"):\n",
    "    print(filename)\n",
    "    d1 = unpickle(filename)\n",
    "    labels = d1[b'labels']\n",
    "    data = d1[b'data']\n",
    "    print(data.shape)\n",
    "    alldata[begin:end,:] = data\n",
    "    alllabels.extend(labels)\n",
    "    begin = end\n",
    "    end = end+number_images_batch\n",
    "alldata = alldata.astype(np.uint8)\n",
    "print(alldata.shape, type(alllabels))\n",
    "print(len(alllabels))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "#learning_rate = 0.01\n",
    "#learning_rate = 0.001\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 512\n",
    "noofbatches = 10\n",
    "\n",
    "ninput = 3072\n",
    "noutput = 10\n",
    "nhidden1 = 512\n",
    "nhidden2 = 256\n",
    "nhidden3 = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomNoise(data):\n",
    "    noise = np.random.normal(0, 3, data.size).reshape([-1, 3072])\n",
    "    data = data + noise\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 60.47020784,  44.45820698,  52.80229117, ..., 138.75301528,\n",
       "         78.72059734,  69.98884811],\n",
       "       [150.89699629, 122.18375752, 104.39244165, ..., 136.67728545,\n",
       "        141.87877197, 140.86316297],\n",
       "       [252.55119394, 253.18141998, 253.2041787 , ...,  86.21418686,\n",
       "         79.72564839,  82.01534999],\n",
       "       ...,\n",
       "       [ 32.94417252,  42.06115004,  43.58377499, ...,  76.34587291,\n",
       "         62.47063509,  44.23388592],\n",
       "       [194.32297041, 186.95180031, 182.0346176 , ..., 172.25381787,\n",
       "        170.05516358, 167.16492175],\n",
       "       [226.5123547 , 238.74990238, 231.61185957, ..., 173.83186012,\n",
       "        165.00423619, 164.71086249]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomNoise(alldata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alldata -> [[ 59  43  50 ... 140  84  72]\n",
      " [154 126 105 ... 139 142 144]\n",
      " [255 253 253 ...  83  83  84]\n",
      " ...\n",
      " [ 35  40  42 ...  77  66  50]\n",
      " [189 186 185 ... 169 171 171]\n",
      " [229 236 234 ... 173 162 161]]\n",
      "[[0.23137255 0.16862746 0.19607843 ... 0.54901963 0.32941177 0.28235295]\n",
      " [0.6039216  0.49411765 0.4117647  ... 0.54509807 0.5568628  0.5647059 ]\n",
      " [1.         0.99215686 0.99215686 ... 0.3254902  0.3254902  0.32941177]\n",
      " ...\n",
      " [0.13725491 0.15686275 0.16470589 ... 0.3019608  0.25882354 0.19607843]\n",
      " [0.7411765  0.7294118  0.7254902  ... 0.6627451  0.67058825 0.67058825]\n",
      " [0.8980392  0.9254902  0.91764706 ... 0.6784314  0.63529414 0.6313726 ]]\n"
     ]
    }
   ],
   "source": [
    "print('alldata -> {}'.format(alldata))\n",
    "data1 = np.array(alldata/255, dtype=np.float32)\n",
    "print(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "alllabels = np.array(alllabels)\n",
    "onehot = np.eye(10)[alllabels]\n",
    "print(onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, [None, ninput])\n",
    "Y = tf.placeholder(tf.float32, [None, noutput])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = \\\n",
    "{\n",
    "        'h1': tf.Variable(tf.random_normal([ninput, nhidden1])),\n",
    "        'h2': tf.Variable(tf.random_normal([nhidden1, nhidden2])),\n",
    "        'h3': tf.Variable(tf.random_normal([nhidden2, nhidden3])),\n",
    "        'out': tf.Variable(tf.random_normal([nhidden3, noutput]))\n",
    "}\n",
    "\n",
    "biases = \\\n",
    "{\n",
    "    'b1': tf.Variable(tf.random_normal([nhidden1])),\n",
    "    'b2': tf.Variable(tf.random_normal([nhidden2])),\n",
    "    'b3': tf.Variable(tf.random_normal([nhidden3])),\n",
    "    'out': tf.Variable(tf.random_normal([noutput]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiperceptron(X):\n",
    "    l1 = tf.nn.sigmoid(tf.add(tf.matmul(X, weights['h1']), biases['b1']))\n",
    "    l2 = tf.nn.sigmoid(tf.add(tf.matmul(l1, weights['h2']), biases['b2']))\n",
    "    l3 = tf.nn.sigmoid(tf.add(tf.matmul(l2, weights['h3']), biases['b3']))\n",
    "    outl = tf.add(tf.matmul(l3, weights['out']), biases['out'])\n",
    "    return outl\n",
    "    \n",
    "model = multiperceptron(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = model, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train_min = optimizer.minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(model, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = tf.summary.FileWriter(\"./logs\", graph=tf.get_default_graph())\n",
    "tf.summary.scalar(\"loss\", loss)\n",
    "\n",
    "tf.summary.histogram(\"weight_1\",weights['h1'])\n",
    "tf.summary.histogram(\"weight_2\",weights['h2'])\n",
    "tf.summary.histogram(\"weight_3\",weights['h3'])\n",
    "tf.summary.scalar(\"loss\", loss)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getbatch(xval, yval, batchsize):\n",
    "    arraylen = len(xval)\n",
    "    count = 0 \n",
    "    while count < arraylen/batchsize:\n",
    "        start = random.randint(0, arraylen-batchsize-1)\n",
    "        count += 1\n",
    "        yield (xval[start:start+batchsize], yval[start:start+batchsize])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 2.3375, Accuracy: 0.0781\n",
      "Epoch: 1, Loss: 2.3220, Accuracy: 0.1016\n",
      "Epoch: 2, Loss: 2.3293, Accuracy: 0.1172\n",
      "Epoch: 3, Loss: 2.3605, Accuracy: 0.1113\n",
      "Epoch: 4, Loss: 2.3670, Accuracy: 0.0898\n",
      "Epoch: 5, Loss: 2.3515, Accuracy: 0.0938\n",
      "Epoch: 6, Loss: 2.3199, Accuracy: 0.1211\n",
      "Epoch: 7, Loss: 2.3580, Accuracy: 0.0898\n",
      "Epoch: 8, Loss: 2.3369, Accuracy: 0.1035\n",
      "Epoch: 9, Loss: 2.3560, Accuracy: 0.1113\n",
      "Testing accuracy: 0.1000\n"
     ]
    }
   ],
   "source": [
    "#0.1\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for (x, y) in getbatch(alldata, onehot, batch_size):\n",
    "            sess.run(train_min, feed_dict={X:x, Y:y})\n",
    "        losscalc, accuracycalc = sess.run([loss, accuracy], feed_dict={X:x, Y:y})\n",
    "        print(\"Epoch: %d, Loss: %0.4f, Accuracy: %0.4f\"%(epoch, losscalc, accuracycalc))\n",
    "            \n",
    "    accuracycalc = sess.run(accuracy, feed_dict={X: alldata, Y: onehot})\n",
    "    print(\"Testing accuracy: %0.4f\"%(accuracycalc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
